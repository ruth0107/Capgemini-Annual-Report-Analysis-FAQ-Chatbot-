import streamlit as st
import pandas as pd
import numpy as np
import nltk 
from nltk.tokenize import word_tokenize 
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import re
import warnings
import logging
import os
import json
import time
import requests 
from io import BytesIO
from pypdf import PdfReader 
import asyncio 

# --- Semantic Dependencies ---
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
# -----------------------------

# --- Configuration & Setup ---
st.set_page_config(
    page_title="PDF RAG Chatbot (TF-IDF)",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Suppress minor warnings
warnings.filterwarnings("ignore", category=FutureWarning)
logging.getLogger("pdfminer").setLevel(logging.ERROR)

# --- NLTK Data Download (FIX FOR LookupError) ---
@st.cache_resource
def download_nltk_data():
    """Ensures necessary NLTK resources are downloaded."""
    try:
        # We still download these, as stopwords and wordnet are required.
        nltk.download('punkt', quiet=True)
        nltk.download('stopwords', quiet=True)
        nltk.download('wordnet', quiet=True)
        st.success("NLTK resources successfully loaded.")
    except Exception as e:
        st.error(f"Failed to download NLTK data: {e}")

download_nltk_data()
# -----------------------------------------------

# Define chunking parameters
CHUNK_SIZE = 500
CHUNK_OVERLAP = 250 

# Define Default PDF Filename
DEFAULT_PDF_FILENAME = "Annual-Report-2024-25.pdf"

# Gemini API Configuration
API_URL = "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-09-2025:generateContent"

# --- NEW: Key Loading Helper ---
def get_initial_api_key():
    """Tries to load API key from Streamlit secrets, then from OS environment."""
    # 1. Check Streamlit secrets
    try:
        if st.secrets.get("GEMINI_API_KEY"):
            return st.secrets["GEMINI_API_KEY"]
    except AttributeError:
        # st.secrets might not exist in local development without secrets.toml
        pass
        
    # 2. Check environment variables
    return os.environ.get("GEMINI_API_KEY", "")
# -------------------------------


# Initialize resources
@st.cache_resource
def setup_nlp_tools():
    """Initializes and caches NLTK resources."""
    stop_words = set(stopwords.words('english'))
    lemmatizer = WordNetLemmatizer()
    return stop_words, lemmatizer

stop_words, lemmatizer = setup_nlp_tools()


# --- Text Processing Functions ---

def basic_clean(text):
    """Preprocess: convert to lower, remove newlines, digits, punctuation, special chars"""
    text = text.lower()
    text = text.replace('\n', ' ')
    text = re.sub(r'\d+', ' ', text)
    text = re.sub(r'[^a-z\s]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def preprocess_words(text, do_lemmatize=True):
    """Word Tokenize (via split), clean, remove stopwords, and lemmatize. Bypasses NLTK Punkt dependency."""
    text = basic_clean(text)
    # FIX: Replaced word_tokenize with simple split to avoid LookupError for 'punkt'
    tokens = text.split() 
    tokens = [t for t in tokens if t not in stop_words and len(t) > 2]
    if do_lemmatize:
        tokens = [lemmatizer.lemmatize(t) for t in tokens]
    return tokens

@st.cache_data
def get_pdf_text_and_chunks(file_data, file_name):
    """
    Extracts text from PDF (BytesIO buffer), cleans it, and splits it into chunks.
    """
    try:
        # 1. Reset file pointer and create BytesIO buffer from raw data
        pdf_reader = PdfReader(BytesIO(file_data))
        
        raw_text = ""
        for page in pdf_reader.pages:
            # Safely extract text, using "" for pages that yield None
            raw_text += page.extract_text() or ""
        
        # 2. Simple Chunking Logic (using raw text for context retrieval)
        cleaned_text = raw_text 
        
        chunks = []
        i = 0
        while i < len(cleaned_text):
            chunk = cleaned_text[i:i + CHUNK_SIZE]
            chunks.append(chunk)
            i += CHUNK_SIZE - CHUNK_OVERLAP 
            
        if not chunks:
            st.error("Could not extract any meaningful text from the PDF.")
            return None, None, None
            
        return raw_text, chunks, file_name
    except Exception as e:
        # Generic error handling for PDF reading issues
        st.error(f"Error reading PDF file: {e}")
        return None, None, None

@st.cache_data
def create_tfidf_index(chunks):
    """
    Creates the TF-IDF Vectorizer and Matrix from the text chunks.
    """
    # Clean chunks for TF-IDF training
    clean_chunks = [" ".join(preprocess_words(chunk, do_lemmatize=True)) for chunk in chunks]

    # Create TF-IDF Vectorizer
    vectorizer = TfidfVectorizer(
        lowercase=False,
        token_pattern=r'(?u)\b[a-zA-Z][a-zA-Z]+\b',
        max_df=0.90, 
        min_df=5
    )
    
    tfidf_matrix = vectorizer.fit_transform(clean_chunks)
    
    return vectorizer, tfidf_matrix, clean_chunks


def get_best_match(query, chunks, vectorizer, tfidf_matrix):
    """
    Retrieves the most relevant chunk using TF-IDF and Cosine Similarity (Semantic Match).
    Includes a keyword boost for high-value terms like 'revenue' or 'total' to improve precision.
    """
    
    # 1. Clean and Transform the Query
    query_tokens = preprocess_words(query, do_lemmatize=True)
    clean_query = " ".join(query_tokens)
    if not clean_query:
        return None, 0.0
    
    # Check if query words exist in the vectorizer's vocabulary
    if not any(token in vectorizer.vocabulary_ for token in query_tokens):
          # Fallback check, though the low score should handle this mostly
          return None, 0.0

    query_vector = vectorizer.transform([clean_query])
    
    # 2. Calculate Cosine Similarity
    cosine_scores = cosine_similarity(query_vector, tfidf_matrix).flatten()
    
    # 3. Apply Keyword Boost (to solve the 'Manpower' problem)
    
    # Define keywords that signal financial facts or specific metrics
    high_value_keywords = ['revenue', 'profit', 'total', 'manpower', 'employees', 'capital', 'divestments', 'subsidiary']
    
    # Create a simple boost array (1.0 for no boost, 1.1 for a boost)
    boost_array = np.ones(len(chunks))
    for i, chunk in enumerate(chunks):
        if any(keyword in chunk.lower() for keyword in high_value_keywords):
            boost_array[i] = 1.15 # Apply a slight, controlled boost (15%)
            
    boosted_scores = cosine_scores * boost_array

    # 4. Find Best Match using boosted score
    best_match_index = np.argmax(boosted_scores)
    best_score = cosine_scores[best_match_index] # Report the true cosine similarity score
    best_chunk = chunks[best_match_index]
        
    return best_chunk, best_score

# --- LLM Generation Function (Synchronous) ---

def generate_answer_with_llm(user_query, context_sentence, retrieval_score, api_key): 
    """
    Calls the Gemini API using the synchronous 'requests' library.
    """
    
    if not api_key: 
        return "**API Key Missing Error:** The RAG Chatbot requires the Gemini API Key to generate conversational answers."

    # 1. Define the system instruction (persona and rules)
    system_instruction = (
        "You are an expert financial analyst chatbot providing concise, conversational answers "
        "based STRICTLY on the provided context, which is a segment retrieved from an Annual Report. "
        "If the context is irrelevant or insufficient, politely state that you cannot answer the question "
        "with the information provided. Do not use external knowledge. "
        "Crucial Rule: If the user asks for a 'Consolidated' figure, ensure the retrieved text explicitly contains 'Consolidated' before answering. If it only contains 'Standalone' or 'Company' figures, you must refuse to answer the 'Consolidated' question, even if the numbers look similar."
    )
    
    # 2. Define the user prompt (question + context)
    prompt = (
        f"Based on the retrieved text below, answer the user's question conversationally.\n\n"
        f"USER QUESTION: {user_query}\n\n"
        f"CONTEXT (from report, relevance score: {retrieval_score:.3f}): {context_sentence}\n\n"
        f"CONVERSATIONAL ANSWER:"
    )

    # 3. Construct the API Payload
    payload = {
        "contents": [{"parts": [{"text": prompt}]}],
        "systemInstruction": {"parts": [{"text": system_instruction}]},
        "generationConfig": {"temperature": 0.2} 
    }

    # 4. API Call with Exponential Backoff
    max_retries = 3
    retry_delay = 1 

    for attempt in range(max_retries):
        try:
            full_api_url = f"{API_URL}?key={api_key}" 
            
            response = requests.post(
                full_api_url,
                headers={'Content-Type': 'application/json'},
                data=json.dumps(payload)
            )
            response.raise_for_status() 
            
            result = response.json()
            
            text = result.get('candidates', [{}])[0].get('content', {}).get('parts', [{}])[0].get('text', 'Error: LLM response missing text.')
            
            # Format the output for the user
            retrieval_info = (
                f"\n\n---\n"
                f"**🤖 RAG Grounding:**\n"
                f"**Relevance Score (Semantic Similarity):** {retrieval_score:.3f}\n"
                f"**Source Context:** *{context_sentence}*"
            )
            return text + retrieval_info

        except requests.exceptions.RequestException as e:
            if attempt < max_retries - 1:
                time.sleep(retry_delay)
                retry_delay *= 2 
            else:
                return f"Sorry, I couldn't connect to the LLM. Network/API Error: {e}"
        except Exception as e:
            return f"Sorry, an unexpected error occurred during processing: {e}"
            
    return "Failed to generate response after multiple retries."


# --- Streamlit UI and Logic ---

def main():
    
    st.title("🧠 RAG Chatbot: Capegemini FAQ's")
    st.markdown("Uploaded PDF is used to build a semantic index for the whole document. Answers are based on **meaningful chunks** from the full report.")
    
    # Initialize API key in session state, prioritizing automatic loading
    if 'gemini_api_key' not in st.session_state:
        st.session_state.gemini_api_key = get_initial_api_key()


    # --- Sidebar for PDF Upload ---
    with st.sidebar:
        st.header("1. Data Source")
        
        file_data = None
        file_name = None
        uploaded_file = None 

        # 1. Check for default file existence (local file path)
        if os.path.exists(DEFAULT_PDF_FILENAME):
            try:
                st.success(f"Default file **{DEFAULT_PDF_FILENAME}** found. Loading automatically.")
                # Read the file content into a byte buffer
                with open(DEFAULT_PDF_FILENAME, "rb") as f:
                    file_data = f.read()
                file_name = DEFAULT_PDF_FILENAME
            except Exception as e:
                st.error(f"Error reading local file: {e}")
                
        # 2. Fallback to manual upload if automatic load fails or file is missing
        if file_data is None:
            st.warning(f"Default file **{DEFAULT_PDF_FILENAME}** not found or failed to load.")
            uploaded_file = st.file_uploader(
                "Upload Capgemini Annual Report PDF", 
                type="pdf",
                key="pdf_uploader", 
                help=f"If you name your file exactly '{DEFAULT_PDF_FILENAME}' and place it next to this script, it will load automatically next time."
            )
            if uploaded_file is not None:
                file_data = uploaded_file.getvalue()
                file_name = uploaded_file.name
                
        st.markdown(f"**Chunk Size:** {CHUNK_SIZE} characters | **Overlap:** {CHUNK_OVERLAP} characters")
        
        st.markdown("---")
        st.header("2. Topics Covered")
        
        # --- UI IMPROVEMENT: Use expander and custom styling ---
        with st.expander("**Optimized Report Sections**", expanded=True):
            st.markdown(
                """
                ### Common Queries & Testing Examples

                **1. Financial Performance**
                - What was the total **Consolidated Profit** for 2024-25?
                - What was the **revenue from operations** for the year?
                
                **2. Human Resources & Governance**
                - What is the total **manpower figure** as of March 31, 2025?
                - How many women employees received benefits under the **Maternity Benefit Act**?
                
                **3. Sustainability & CSR**
                - Explain the goal of the **'Mission Million Trees'** project.
                - What technology is used in the **Battery Energy Storage Solution (BESS)**?
                """
            )
        # --- END UI IMPROVEMENT ---
        
        st.markdown("---")
        st.header("3. Gemini API Key") 
        
        # Get the current operational API key
        current_api_key = st.session_state.gemini_api_key
        
        # Logic to handle key display based on whether it was loaded automatically
        if current_api_key and get_initial_api_key():
            # Key loaded from secrets or env
            st.success("✅ Gemini API Key successfully loaded from secrets or environment.")
        else:
            # Fallback to manual input if no key is found automatically or if previous key failed
            key_input = st.text_input(
                "Enter your Gemini API Key:",
                type="password",
                value=current_api_key, # Use current_api_key which might be a manually entered value from a previous run
                key="api_key_input",
                help="You must enter your key if it's not set via environment variables or secrets.toml."
            )
            # Update session state with the current input key
            st.session_state.gemini_api_key = key_input
            
            # Re-check and warn if still missing
            if not st.session_state.gemini_api_key:
                st.warning("⚠️ **API Key Missing!** Please enter your Gemini API Key above to enable the chat.")


    # --- Data Loading and Indexing ---
    
    # Get the final operational API key after checking sidebar input
    current_api_key = st.session_state.gemini_api_key
    
    data_source_key = file_name 
    
    if file_data is None:
        st.warning("Please upload a PDF or ensure the default file is present to begin the analysis.")
        return

    # Process PDF into Chunks
    raw_text, chunks, processed_file_name = get_pdf_text_and_chunks(file_data, file_name)
    if chunks is None:
        return 

    # Create Semantic Index
    with st.spinner(f"Indexing {processed_file_name} into {len(chunks)} chunks for semantic retrieval..."):
        # This builds the TF-IDF vectorizer and matrix
        vectorizer, tfidf_matrix, clean_chunks = create_tfidf_index(chunks)
    
    st.success(f"Indexed {processed_file_name} into {len(chunks)} chunks. Ready for chat.")

    if vectorizer is None:
        return
        
    # Chat Initialization
    if "messages" not in st.session_state:
        st.session_state.messages = []
        st.session_state.messages.append({"role": "assistant", "content": f"The full report ({processed_file_name}) has been indexed. Ask me about **consolidated revenue** or **CSR projects**."})

    # Display Chat History
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # --- CHAT INPUT LOGIC ---
    if current_api_key:
        user_query = st.chat_input("Ask a question about the full report...")
    else:
        # Display a disabled input box when the key is missing to prevent chat submission errors
        user_query = st.chat_input("Please enter your Gemini API Key in the sidebar to chat...", disabled=True)
        # We return here only if there is no user_query, preventing the RAG pipeline from running
        return


    if user_query:
        # Add user message to history and display
        st.session_state.messages.append({"role": "user", "content": user_query})
        with st.chat_message("user"):
            st.markdown(user_query)

        # RAG Pipeline Execution
        with st.chat_message("assistant"):
            # 1. RETRIEVE: Get the best context chunk using TF-IDF/Cosine Similarity
            best_chunk, best_score = get_best_match(user_query, chunks, vectorizer, tfidf_matrix)
            
            # Use a threshold to filter out poor matches (score is generally low for TF-IDF)
            if best_chunk is None or best_score < 0.1: 
                 st.error("I couldn't find a strongly relevant chunk in the report (Score too low). Please try rephrasing with specific keywords.")
                 st.session_state.messages.append({"role": "assistant", "content": "I couldn't find a strongly relevant chunk in the report. Please try rephrasing with specific keywords."})
                 return

            # 2. GENERATE: Pass context to LLM
            with st.spinner(f"Retrieving context (Score: {best_score:.3f})... Generating conversational answer with Gemini LLM..."):
                response_text = generate_answer_with_llm(
                    user_query, 
                    best_chunk, 
                    best_score,
                    current_api_key # <-- PASS THE OPERATIONAL KEY
                )
                st.markdown(response_text)

        # Add assistant response to history
        st.session_state.messages.append({"role": "assistant", "content": response_text})

if __name__ == "__main__":
    main()
